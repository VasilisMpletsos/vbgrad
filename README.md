# VB Grad

[![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=fff)](#)
[![NumPy](https://img.shields.io/badge/NumPy-4DABCF?logo=numpy&logoColor=fff)](#)
[![Matplotlib](https://custom-icon-badges.demolab.com/badge/Matplotlib-71D291?logo=matplotlib&logoColor=fff)](#)
[![PyTorch](https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white)](#)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-ff8f00?logo=tensorflow&logoColor=white)](#)

VB Grad is a simple library for automatic differentiation similarly like many used under the hood in various famous libraries for AI like Pytorch with autograd or Tenforflow.

## ðŸš€ About This Project

This project follows Andrej Karpathyâ€™s tutorial ([Youtube Link](https://youtu.be/VMj-3S1tku0?si=gwRM_HmXrDNS4A5W) , [Micrograd Library](https://github.com/karpathy/micrograd/tree/master)) on building an automatic differentiation engine from scratch.

Although automatic differentiation is one of the most fundamental aspects of neural networks and the building stone of optimization processes, many engineers, including myself, lack a deep understanding of what actually happens at lower levels.

I found this lecture extremely valuable, so I decided to follow it closely and build my own automatic differentiation engine to gain a deeper, hands-on understanding of how the "universe" works.
